{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import Library",
   "id": "f6f2b36b4bdbbc9f"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-10T14:55:31.374755Z",
     "start_time": "2025-12-10T14:55:29.531417Z"
    }
   },
   "source": [
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "try:\n",
    "    stopwords.words('english')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "try:\n",
    "    word_tokenize(\"test\")\n",
    "except LookupError:\n",
    "    nltk.download('punkt')\n",
    "\n",
    "# TextBlob\n",
    "from textblob import TextBlob\n",
    "\n",
    "# spaCy\n",
    "import spacy\n",
    "try:\n",
    "    nlp_spacy = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"Warning: spaCy model 'en_core_web_sm' not found. Please run 'python -m spacy download en_core_web_sm'\")\n",
    "    nlp_spacy = None"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load text from alice29.txt",
   "id": "6f2e955bec3a5d8b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T14:53:12.923241Z",
     "start_time": "2025-12-10T14:53:12.918659Z"
    }
   },
   "cell_type": "code",
   "source": [
    "file_path = \"alice29.txt\"\n",
    "with open('alice29.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()"
   ],
   "id": "d33dc4b191cc28c6",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Text Cleaning",
   "id": "6acadc0a2dce6b5d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:01:55.857082Z",
     "start_time": "2025-12-10T18:01:55.838377Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def clean_text_and_remove_stopwords(input_text):\n",
    "\n",
    "    cleaned_text = re.sub(r'[^a-zA-Z0-9\\s]', ' ', input_text)\n",
    "\n",
    "    cleaned_text = cleaned_text.lower()\n",
    "\n",
    "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
    "\n",
    "    english_stopwords = set(stopwords.words('english'))\n",
    "\n",
    "    words = cleaned_text.split()\n",
    "    filtered_words = [word for word in words if word not in english_stopwords and word.strip()]\n",
    "\n",
    "    final_cleaned_text = ' '.join(filtered_words)\n",
    "\n",
    "    return final_cleaned_text, filtered_words\n",
    "\n",
    "cleaned_text, all_filtered_words = clean_text_and_remove_stopwords(text)\n",
    "\n",
    "with open('cleaned.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(cleaned_text)\n",
    "\n",
    "print(cleaned_text[:500] + \"...\")"
   ],
   "id": "7d7009238f82a9cf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alice adventures wonderland lewis carroll millennium fulcrum edition 2 9 chapter rabbit hole alice beginning get tired sitting sister bank nothing twice peeped book sister reading pictures conversations use book thought alice without pictures conversation considering mind well could hot day made feel sleepy stupid whether pleasure making daisy chain would worth trouble getting picking daisies suddenly white rabbit pink eyes ran close nothing remarkable alice think much way hear rabbit say oh dea...\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Tokenization",
   "id": "97b3764682dbe4c1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T18:02:03.811880Z",
     "start_time": "2025-12-10T18:02:03.735049Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sentences = sent_tokenize(text)\n",
    "words_from_original_text = word_tokenize(text)\n",
    "\n",
    "with open('words.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(\"--- Tokenized Sentences ---\\n\")\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        f.write(f\"Sentence {i+1}: {sentence}\\n\")\n",
    "\n",
    "    f.write(\"\\n--- Tokenized Words ---\\n\")\n",
    "    f.write(', '.join(words_from_original_text))\n",
    "\n",
    "    f.write(\"\\n\\n--- Filtered Words (Lowercase, No Stopwords, No Punctuation) ---\\n\")\n",
    "    f.write(', '.join(all_filtered_words))\n",
    "\n",
    "\n",
    "print(f\"Total Sentences (NLTK): {len(sentences)}\")\n",
    "print(f\"Total Words (NLTK, including punctuation): {len(words_from_original_text)}\")\n",
    "print(f\"Total Filtered Words (Cleaned): {len(all_filtered_words)}\")"
   ],
   "id": "46d1de73be3dc909",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Sentences (NLTK): 1614\n",
      "Total Words (NLTK, including punctuation): 34428\n",
      "Total Filtered Words (Cleaned): 12244\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Frequency Analysis",
   "id": "ec4272a936606d0d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T15:40:01.460419Z",
     "start_time": "2025-12-10T15:40:01.456445Z"
    }
   },
   "cell_type": "code",
   "source": [
    "word_counts = Counter(all_filtered_words)\n",
    "top_10_words = word_counts.most_common(10)\n",
    "\n",
    "top_10_table = \"--- Top 10 Most Frequent Words ---\\n\"\n",
    "top_10_table += \"{:<15} {:<5}\\n\".format(\"Word\", \"Count\")\n",
    "top_10_table += \"-\" * 20 + \"\\n\"\n",
    "for word, count in top_10_words:\n",
    "    top_10_table += \"{:<15} {:<5}\\n\".format(word, count)\n",
    "\n",
    "with open('top10words.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(top_10_table)\n",
    "\n",
    "print(top_10_table)"
   ],
   "id": "9eac53b2a6ce97ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Top 10 Most Frequent Words ---\n",
      "Word            Count\n",
      "--------------------\n",
      "said            462  \n",
      "alice           398  \n",
      "little          128  \n",
      "one             104  \n",
      "know            88   \n",
      "like            85   \n",
      "would           83   \n",
      "went            83   \n",
      "could           77   \n",
      "queen           75   \n",
      "\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Framework Performance Comparison",
   "id": "c47ea452f1a6ba85"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-10T17:24:53.822439Z",
     "start_time": "2025-12-10T17:23:58.396674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import timeit\n",
    "import spacy\n",
    "\n",
    "try:\n",
    "    nlp_spacy = spacy.load(\"en_core_web_sm\", disable=[\"tagger\", \"parser\", \"ner\"])\n",
    "    print(\"spaCy model loaded (tokenization-only) successfully.\")\n",
    "except OSError:\n",
    "    print(\"Error: spaCy model 'en_core_web_sm' not found.\")\n",
    "    nlp_spacy = None\n",
    "\n",
    "NUM_EXECUTIONS = 50\n",
    "NUM_REPEATS = 5\n",
    "results = {}\n",
    "\n",
    "print(\"\\n--- Comparing Framework Performance (Word Tokenization) using timeit ---\")\n",
    "\n",
    "SETUP_NLTK = \"\"\"\n",
    "from nltk.tokenize import word_tokenize\n",
    "text = globals()['text']\n",
    "\"\"\"\n",
    "STMT_NLTK = \"word_tokenize(text)\"\n",
    "\n",
    "t_nltk = timeit.repeat(\n",
    "    setup=SETUP_NLTK,\n",
    "    stmt=STMT_NLTK,\n",
    "    repeat=NUM_REPEATS,\n",
    "    number=NUM_EXECUTIONS,\n",
    "    globals={'text': text}\n",
    ")\n",
    "\n",
    "avg_time_nltk = min(t_nltk) / NUM_EXECUTIONS\n",
    "results['NLTK'] = avg_time_nltk\n",
    "print(f\"NLTK Time: {avg_time_nltk:.6f} seconds \")\n",
    "\n",
    "\n",
    "SETUP_TEXTBLOB = \"\"\"\n",
    "from textblob import TextBlob\n",
    "text = globals()['text']\n",
    "\"\"\"\n",
    "STMT_TEXTBLOB = \"TextBlob(text).words\"\n",
    "\n",
    "t_textblob = timeit.repeat(\n",
    "    setup=SETUP_TEXTBLOB,\n",
    "    stmt=STMT_TEXTBLOB,\n",
    "    repeat=NUM_REPEATS,\n",
    "    number=NUM_EXECUTIONS,\n",
    "    globals={'text': text}\n",
    ")\n",
    "\n",
    "avg_time_textblob = min(t_textblob) / NUM_EXECUTIONS\n",
    "results['TextBlob'] = avg_time_textblob\n",
    "print(f\"TextBlob Time: {avg_time_textblob:.6f} seconds \")\n",
    "\n",
    "if nlp_spacy:\n",
    "    SETUP_SPACY = \"\"\"\n",
    "nlp_spacy = globals()['nlp_spacy']\n",
    "text = globals()['text']\n",
    "\"\"\"\n",
    "    STMT_SPACY = \"doc = nlp_spacy.make_doc(text); [t.text for t in doc]\"\n",
    "\n",
    "    t_spacy = timeit.repeat(\n",
    "        setup=SETUP_SPACY,\n",
    "        stmt=STMT_SPACY,\n",
    "        repeat=NUM_REPEATS,\n",
    "        number=NUM_EXECUTIONS,\n",
    "        globals={'text': text, 'nlp_spacy': nlp_spacy}\n",
    "    )\n",
    "\n",
    "    avg_time_spacy = min(t_spacy) / NUM_EXECUTIONS\n",
    "    results['spaCy'] = avg_time_spacy\n",
    "    print(f\"spaCy Time: {avg_time_spacy:.6f} seconds \")\n",
    "\n",
    "\n",
    "compare_table = (\n",
    "    f\"--- Framework Performance Comparison \"\n",
    "    f\"(Avg. Word Tokenization Time over {NUM_EXECUTIONS} executions) ---\\n\"\n",
    ")\n",
    "compare_table += \"{:<10} {:<15}\\n\".format(\"Framework\", \"Time (Seconds)\")\n",
    "compare_table += \"-\" * 25 + \"\\n\"\n",
    "\n",
    "for framework, t in results.items():\n",
    "    compare_table += \"{:<10} {:<15.6f}\\n\".format(framework, t)\n",
    "\n",
    "compare_table += \"-\" * 25 + \"\\n\"\n",
    "\n",
    "with open('time_compare.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(compare_table)\n",
    "\n"
   ],
   "id": "f03c42e4ebb0b9ed",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy model loaded (tokenization-only) successfully.\n",
      "\n",
      "--- Comparing Framework Performance (Word Tokenization) using timeit ---\n",
      "NLTK Time: 0.049032 seconds \n",
      "TextBlob Time: 0.070582 seconds \n",
      "spaCy Time: 0.100722 seconds \n"
     ]
    }
   ],
   "execution_count": 16
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
